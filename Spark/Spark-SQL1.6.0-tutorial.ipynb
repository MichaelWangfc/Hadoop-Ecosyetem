{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# import pandas as pd\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "pd.options.display.mpl_style = 'default'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL\n",
    "Spark SQL is a Spark module for structured data processing\n",
    "![IPython](Image/sparksql-log1.jpg \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "- Spark SQL features\n",
    "- Spark SQL flow dagram\n",
    "- Spark SQL architecture\n",
    "- Spark SQL libaries\n",
    "\n",
    "- Spark SQL components\n",
    "- SchemaRDD\n",
    "\n",
    "- Spark SQL example - json file\n",
    "\n",
    "- Spark SQL EXAMPLE--Oracle database\n",
    "\n",
    "- Running SQL Queries Programmatically\n",
    "\n",
    "- Spark SQL VS Hive\n",
    "\n",
    "\n",
    "referecne:\n",
    "https://spark.apache.org/sql/\n",
    "https://spark.apache.org/docs/1.6.0/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analytics the Old Way\n",
    "Put the data into an RDBMS\n",
    "\n",
    "- Pros: High level query language, optimized execution engine\n",
    "\n",
    "- Cons: Have to ETL the data in, some analysis is hard to do in SQL, doesn’t scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Analytics the New Way\n",
    "Map/Reduce, etc\n",
    "- Pros: Full featured programming language,\n",
    "easy parallelism\n",
    "- Cons: Difficult to do ad-hoc analysis,\n",
    "optimizations are left up to the developer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### SQL on HDFS\n",
    "Put the data into a （not RDBMS） ***HDFS***\n",
    "- Pros: High level query language, optimized execution engine\n",
    "- Cons: (Have to ETL the data in,） some analysis is hard to do in SQL,（ doesn’t scale HDFS）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL features\n",
    "![IPython](Image/sparksql-feature1.jpg \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![IPython](Image/sparksql-feature2.jpg \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![IPython](Image/sparksql-feature3.jpg \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark SQL architecture\n",
    "![IPython](Image/sparksql-architecture1.jpg \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![IPython](Image/sparksql-flow-diagram2.jpg \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark SQL Libararies\n",
    "![IPython](Image/sparksql-lib1.png \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark SQL data source API\n",
    "![IPython](Image/sparksql-datasource-api1.png \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " Spark SQL data frame API\n",
    "![IPython](Image/sparksql-dataframe-api1.jpg \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark SQL Interpreter and Optimizer\n",
    "![IPython](Image/sparksql-interpreter1.jpg \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark SQL Service\n",
    "![IPython](Image/sparksql-service1.png \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL components\n",
    "![IPython](Image/sparksql-components1.png \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SchemaRDD\n",
    "Adding Schema to RDDs\n",
    "![IPython](Image/sparksql-schema-rdd1.png \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL EXAMPLE--json file\n",
    "1. 启动local / standalone cluster and pyspark shell\n",
    "2. Initializing Spark - SparkContext object\n",
    "3. Create a basic SQLContext\n",
    "4. Creating DataFrames\n",
    "5. DataFrame Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1. 启动local model and pyspark shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#一般以 Spark 的安装目录（/usr/local/spark）为当前路径\n",
    "$ cd /usr/local/spark\n",
    "\n",
    "#run pyspark shell as local mode\n",
    "$ ./bin/pyspark\n",
    "#不需要加任何参量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 启动standalone cluster and pyspark shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#一般以 Spark 的安装目录（/usr/local/spark）为当前路径\n",
    "$ cd /usr/local/spark\n",
    "\n",
    "#start the cluster by script\n",
    "$ ./sbin/start-all.sh \n",
    "\n",
    "\n",
    "#to launch an interactive Python shell，and you must define the master node \n",
    "# (do not foreget the master argument!!)\n",
    "$ ./bin/pyspark  --master  spark://10.9.1.56:7077\n",
    "\n",
    "\n",
    "# Stops both the master and the slaves as described above.\n",
    "$ ./sbin/stop-all.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2. Initializing Spark - SparkContext object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The first thing a Spark program must do is to create a SparkContext object, \n",
    "# which tells Spark how to access a cluster.\n",
    "\n",
    "# To create a SparkContext you first need to build a SparkConf object that contains \n",
    "# information about your application.\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# master is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in \n",
    "#local mode. \n",
    "#In practice, when running on a cluster, you will not want to hardcode master in the \n",
    "#program, but rather launch the application with spark-submit and receive it there\n",
    "\n",
    "#python defalt for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3. create a basic SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#The entry point into all relational functionality in Spark is the SQLContext class, or one of its decedents\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 4 Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#The following creates a DataFrame based on the content of a JSON file:\n",
    "df = sqlContext.read.json(\"file:///usr/local/spark/examples/src/main/resources/people.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n",
    "\n",
    "## age  name\n",
    "## null Michael\n",
    "## 30   Andy\n",
    "## 19   Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 5. DataFrame Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print the schema in a tree format\n",
    "df.printSchema()\n",
    "## root\n",
    "## |-- age: long (nullable = true)\n",
    "## |-- name: string (nullable = true)\n",
    "\n",
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "## name\n",
    "## Michael\n",
    "## Andy\n",
    "## Justin\n",
    "\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "## name    (age + 1)\n",
    "## Michael null\n",
    "## Andy    31\n",
    "## Justin  20\n",
    "\n",
    "# Select people older than 21\n",
    "df.filter(df['age'] > 21).show()\n",
    "## age name\n",
    "## 30  Andy\n",
    "\n",
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "## age  count\n",
    "## null 1\n",
    "## 19   1\n",
    "## 30   1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Connecting to SQL Databases using JDBC\n",
    "Spark SQL EXAMPLE--Oracle database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. launch local / standalone cluster and pyspark shell\n",
    "2. Initializing Spark - SparkContext object\n",
    "3. Create a basic SQLContext\n",
    "4. Set Spark environment\n",
    "5. Using Spark SQL and Spark Shell (Pyspark /Scala)\n",
    "5. Creating DataFrames\n",
    "6. DataFrame Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    " 1. launch standalone cluster model and pyspark shell\n",
    "#一般以 Spark 的安装目录（/usr/local/spark）为当前路径\n",
    "$ cd /usr/local/spark\n",
    "#start the cluster by script\n",
    "$ ./sbin/start-all.sh \n",
    "#to launch an interactive Python shell，and you must define the master node \n",
    "# (do not foreget the master argument!!)\n",
    "$ ./bin/pyspark  --master  spark://10.9.1.56:7077\n",
    "# Stops both the master and the slaves as described above.\n",
    "$ ./sbin/stop-all.sh \n",
    "\n",
    "2. Initializing Spark - SparkContext object\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "3. create a basic SQLContext\n",
    "#The entry point into all relational functionality in Spark is the SQLContext class, or one of its decedents\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  4.Set Spark environment\n",
    "the JDBC driver for you particular database on the spark classpath.\n",
    "\n",
    "Our next step will be providing the Spark environment with the classpath for the JDBC driver we’re going to use. \n",
    "I used Oracle 10g I had installed on that server – but we can use any driver and any system we like here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "export SPARK_CLASSPATH=C:\\oracle\\product\\10.2.0\\client_1\\jdbc\\lib\\ojdbc4.jar\n",
    "    \n",
    "#### 5 .Using Spark SQL and Spark Shell (Pyspark /Scala)\n",
    "$ ./bin/pyspark\n",
    "\n",
    "\n",
    "# or combine the two steps\n",
    "SPARK_CLASSPATH=C:\\oracle\\product\\10.2.0\\client_1\\jdbc\\lib\\ojdbc4.jar ./bin/spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#### 6. create a data frame:\n",
    "#for pyspark\n",
    "df = sqlContext.read.format('jdbc').options(\n",
    "                                url='jdbc:postgresql:dbserver', \n",
    "                                dbtable='schema.tablename').load()\n",
    "\n",
    "\n",
    "#for scala\n",
    "scala> val employees = sqlContext.load(\"jdbc\", \n",
    "    Map(\"url\" -> \"jdbc:oracle:thin:zohar/zohar@//localhost:1521/single\", \n",
    "    \"dbtable\" -> \"hr.employees\"))\n",
    "\n",
    "val jdbcDF = sqlContext.read.format(\"jdbc\").options(\n",
    "  Map(\"url\" -> \"jdbc:postgresql:dbserver\",\n",
    "  \"dbtable\" -> \"schema.tablename\")).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#### 7.Dataframe operation\n",
    "\n",
    "scala> employees.count()\n",
    "scala> employees.printSchema\n",
    "scala> employees.show\n",
    "scala> employees.filter(\"EMPLOYEE_ID = 101\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running SQL Queries Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.sql(\"SELECT * FROM table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### RDDs into Relations (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a dictionary.\n",
    "lines = sc.textFile(\"examples/…/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p:{\"name\": p[0],\"age\": int(p[1])})\n",
    "\n",
    "# Infer the schema, and register the SchemaRDD as a table\n",
    "peopleTable = sqlCtx.inferSchema(people)\n",
    "peopleTable.registerAsTable(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark SQL VS Hive\n",
    "![IPython](Image/sparksql-vs-hive1.jpg \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark SQL VS Hive\n",
    "![IPython](Image/sparksql-vs-hive2.jpg \"IPython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

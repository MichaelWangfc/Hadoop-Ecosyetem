{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark application (instance of SparkContext)\n",
    "\n",
    "Application\n",
    "  \n",
    "  - a driver program \n",
    "  - executors on the cluster.\n",
    "   (parallel operations)\n",
    "\n",
    "### The main abstraction:\n",
    "1. resilient distributed dataset (RDD),\n",
    "\n",
    "2. shared variables\n",
    "\n",
    " - broadcast variables\n",
    " - accumulators\n",
    " \n",
    "### Deploy mode\t\n",
    "Distinguishes where the driver process runs.\n",
    "- In \"cluster\" mode, the framework launches the driver inside of the cluster. \n",
    "- In \"client\" mode, the submitter launches the driver outside of the cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin/spark-submit \n",
    "#script located in the Spark directory. This script will load Spark’s Java/Scala libraries \n",
    "#and allow you to submit applications to a cluster. \n",
    "\n",
    "bin/pyspark\n",
    "#to launch an interactive Python shell.\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "#import some Spark classes into your program\n",
    "\n",
    "$ PYSPARK_PYTHON=python3.4 bin/pyspark\n",
    "$ PYSPARK_PYTHON=/opt/pypy-2.5/bin/pypy bin/spark-submit examples/src/main/python/pi.py\n",
    "# It uses the default python version in PATH, you can specify which version of Python you want to use by \n",
    "# PYSPARK_PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first thing a Spark program must do is to create a SparkContext object, \n",
    "# which tells Spark how to access a cluster.\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    " \n",
    "\n",
    "# To create a SparkContext you first need to build a SparkConf object that contains \n",
    "# information about your application.\n",
    "\n",
    "???\n",
    "# master is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in \n",
    "#local mode. \n",
    "#In practice, when running on a cluster, you will not want to hardcode master in the \n",
    "#program, but rather launch the application with spark-submit and receive it there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using the Shell--PySpark shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In the PySpark shell, a special interpreter-aware SparkContext is already created for you,\n",
    "in the variable called sc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For example, to run bin/pyspark on exactly four cores,\n",
    "\n",
    "$ ./bin/pyspark --master local[4] --py-files code.py\n",
    "\n",
    "# bin/pyspark\n",
    "#to launch an interactive Python shell.\n",
    "# --master argument: \n",
    "#which master the context connects to using\n",
    "# --py-files\n",
    "#add Python .zip, .egg or .py files to the runtime path by passing a comma-separated list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyspark --help\n",
    "#for help\n",
    "\n",
    "#launch the PySpark shell in IPython\n",
    "#To use IPython, set the PYSPARK_DRIVER_PYTHON variable to ipython when running bin/pyspark:\n",
    "$ PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark\n",
    "\n",
    "#To use the Jupyter notebook (previously known as the IPython notebook)\n",
    "$ PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=notebook ./bin/pyspark\n",
    "\n",
    "# （input the command %pylab inline as part of your notebook before you start to try Spark \n",
    "# from the Jupyter notebook.）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets (RDDs)\n",
    "a fault-tolerant collection of elements that can be operated on in parallel. \n",
    "\n",
    "two ways to create RDDs: \n",
    " - parallelizing an existing collection in your driver program, \n",
    " - or referencing a dataset in an external storage system:\n",
    "    \n",
    "    such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Text file RDDs can be created using SparkContext’s textFile method. \n",
    "#This method takes an URI for the file (either a local path on the machine, or \n",
    "#a hdfs://, s3n://, etc URI) and reads it as a collection of lines.\n",
    "distFile = sc.textFile(\"data.txt\")\n",
    "\n",
    "#add up the sizes of all the lines using the map and reduce operations\n",
    "distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For example, here is how to create a parallelized collection holding the numbers 1 to 5:\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data,10)\n",
    "# the number of partitions: 10\n",
    "#One important parameter for parallel collections is the number of partitions to cut the \n",
    "#dataset into. \n",
    "#Spark will run one task for each partition of the cluster. \n",
    "#Typically you want 2-4 partitions for each CPU in your cluster.\n",
    "\n",
    "# add up the elements of the list.\n",
    "distData.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Operations\n",
    " - transformations:\n",
    "   \n",
    "   which create a new dataset from an existing one\n",
    "   \n",
    "   \n",
    " - actions (“jobs” (Spark actions) )\n",
    "  \n",
    "  which return a value to the driver program after running a computation on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defines a base RDD from an external file\n",
    "lines = sc.textFile(\"data.txt\")\n",
    "\n",
    "#defines lineLengths as the result of a map transformation(lazy).\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "\n",
    "#lineLengths.persist()\n",
    "#cause lineLengths to be saved in memory after the first time it is computed.\n",
    "\n",
    "#run reduce, which is an action. At this point Spark breaks the computation into tasks \n",
    "#to run on separate machines, and each machine runs both its part of the map and a \n",
    "#local reduction, returning only its answer to the driver program.\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Functions to Spark\n",
    "- Lambda expressions\n",
    "- Local defs inside the function calling into Spark, for longer code.\n",
    "- Top-level functions in a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"MyScript.py\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    def myFunc(s):\n",
    "        words = s.split(\" \")\n",
    "        return len(words)\n",
    "\n",
    "    sc = SparkContext(...)\n",
    "    sc.textFile(\"file.txt\").map(myFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-07b91afb836c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-07b91afb836c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    The spark-submit script in Spark’s bin directory\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The spark-submit script in Spark’s bin directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bundling Your Application’s Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-326a7989e57a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-326a7989e57a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    create an assembly jar (or “uber” jar) containing your code and its dependencies\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "create an assembly jar (or “uber” jar) containing your code and its dependencies\n",
    "\n",
    "For Python, you can use the \n",
    "--py-files argument of spark-submit \n",
    "to add .py, .zip or .egg files to be distributed with your application. \n",
    "If you depend on multiple Python files we recommend packaging them into a .zip or .egg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### launch the application with spark-submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-333507bd1494>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-333507bd1494>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ./bin/spark-submit   --class <main-class>   --master <master-url>   --deploy-mode <deploy-mode>   --conf <key>=<value>   ... # other options\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "./bin/spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring\n",
    "Every SparkContext launches a web UI, by default on port 4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://<driver-node>:4040\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python27]",
   "language": "python",
   "name": "conda-env-python27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
